{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import skipgrams, pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'total m = 159571'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "df_train = pd.read_csv('input/train.csv')\n",
    "\n",
    "x_train = df_train['comment_text']\n",
    "y_train = df_train[categories].as_matrix()\n",
    "\n",
    "display(df_train.head())\n",
    "m = x_train.shape[0]\n",
    "display(f'total m = {m}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 300\n",
    "epochs = 30\n",
    "one_weight = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "\n",
    "f = open('input/glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    w = values[0]\n",
    "    weights = np.asarray(values[1:], dtype='float32')\n",
    "    embedding_index[w] = weights\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length 1403\n",
      "min length 1\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(lower=True)\n",
    "tokenizer.fit_on_texts(texts=x_train)\n",
    "\n",
    "vocabulary_size = len(tokenizer.word_index)\n",
    "sequences = tokenizer.texts_to_sequences(texts=x_train)\n",
    "\n",
    "max_seq_len = 0\n",
    "for sequence in sequences:\n",
    "    max_seq_len = max(max_seq_len, len(sequence))\n",
    "\n",
    "print('max length {}'.format(max_seq_len))\n",
    "\n",
    "min_seq_len = max_seq_len\n",
    "for sequence in sequences:\n",
    "    min_seq_len = min(min_seq_len, len(sequence))\n",
    "    \n",
    "print('min length {}'.format(min_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.221569082101382"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(x) for x in sequences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "\n",
    "embedding_matrix = np.zeros((vocabulary_size + 1, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.9883534256840718, 4.248940284069297, 2.5817659291353974, 5.4539585667967865, 2.6518668995849177, 4.375776717520396]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.layers import Bidirectional, LSTM, GRU, Dense\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "embedding = Embedding(vocabulary_size + 1, embedding_dim, weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "def init_weighted_binary_crossentropy(one_weights, zero_weights):\n",
    "\n",
    "    def weight_binary_crossentropy(y_true, y_pred):\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), (1.0 - K.epsilon()))\n",
    "        \n",
    "        o_terms = tf.multiply(tf.multiply(y_true, tf.log(y_pred)), one_weights)\n",
    "        z_terms = tf.multiply(tf.subtract(1.0, y_true), tf.log(tf.subtract(1.0, y_pred)))\n",
    "\n",
    "        terms = tf.add(o_terms, z_terms)\n",
    "        terms = tf.negative(K.mean(terms))\n",
    "\n",
    "        return K.mean(terms)\n",
    "    \n",
    "    return weight_binary_crossentropy\n",
    "\n",
    "input_model = Sequential()\n",
    "input_model.add(embedding)\n",
    "input_model.add(Bidirectional(LSTM(32)))\n",
    "input_model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "c_count = [15294, 1595, 8449, 478, 7877, 1405]\n",
    "o_weights = [max(1.0, math.log(one_weight * m / float(c))) for c in c_count]\n",
    "print(o_weights)\n",
    "z_weights = np.zeros(6)\n",
    "\n",
    "input_model.compile(optimizer='adam', loss=init_weighted_binary_crossentropy(o_weights, z_weights), metrics=[categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "159571/159571 [==============================] - 845s 5ms/step - loss: 0.4250 - categorical_accuracy: 0.1718\n",
      "Epoch 2/30\n",
      "159571/159571 [==============================] - 806s 5ms/step - loss: 0.2315 - categorical_accuracy: 0.8251\n",
      "Epoch 3/30\n",
      "159571/159571 [==============================] - 801s 5ms/step - loss: 0.1588 - categorical_accuracy: 0.7245\n",
      "Epoch 4/30\n",
      "159571/159571 [==============================] - 829s 5ms/step - loss: 0.1308 - categorical_accuracy: 0.9300\n",
      "Epoch 5/30\n",
      "159571/159571 [==============================] - 818s 5ms/step - loss: 0.1177 - categorical_accuracy: 0.9704\n",
      "Epoch 6/30\n",
      "159571/159571 [==============================] - 806s 5ms/step - loss: 0.1101 - categorical_accuracy: 0.9754\n",
      "Epoch 7/30\n",
      "159571/159571 [==============================] - 806s 5ms/step - loss: 0.1049 - categorical_accuracy: 0.9680\n",
      "Epoch 8/30\n",
      "159571/159571 [==============================] - 807s 5ms/step - loss: 0.1012 - categorical_accuracy: 0.9677\n",
      "Epoch 9/30\n",
      "159571/159571 [==============================] - 810s 5ms/step - loss: 0.0983 - categorical_accuracy: 0.9809\n",
      "Epoch 10/30\n",
      "159571/159571 [==============================] - 811s 5ms/step - loss: 0.0947 - categorical_accuracy: 0.9845\n",
      "Epoch 11/30\n",
      "159571/159571 [==============================] - 813s 5ms/step - loss: 0.0924 - categorical_accuracy: 0.9770\n",
      "Epoch 12/30\n",
      "159571/159571 [==============================] - 800s 5ms/step - loss: 0.0907 - categorical_accuracy: 0.9789\n",
      "Epoch 13/30\n",
      "159571/159571 [==============================] - 805s 5ms/step - loss: 0.0889 - categorical_accuracy: 0.9838\n",
      "Epoch 14/30\n",
      "159571/159571 [==============================] - 827s 5ms/step - loss: 0.0869 - categorical_accuracy: 0.9861\n",
      "Epoch 15/30\n",
      "159571/159571 [==============================] - 813s 5ms/step - loss: 0.0856 - categorical_accuracy: 0.9871\n",
      "Epoch 16/30\n",
      "159571/159571 [==============================] - 806s 5ms/step - loss: 0.0847 - categorical_accuracy: 0.9858\n",
      "Epoch 17/30\n",
      "159571/159571 [==============================] - 816s 5ms/step - loss: 0.0841 - categorical_accuracy: 0.9826\n",
      "Epoch 18/30\n",
      "159571/159571 [==============================] - 802s 5ms/step - loss: 0.0818 - categorical_accuracy: 0.9864\n",
      "Epoch 19/30\n",
      "159571/159571 [==============================] - 830s 5ms/step - loss: 0.0808 - categorical_accuracy: 0.9887\n",
      "Epoch 20/30\n",
      "159571/159571 [==============================] - 831s 5ms/step - loss: 0.0794 - categorical_accuracy: 0.9870\n",
      "Epoch 21/30\n",
      "159571/159571 [==============================] - 805s 5ms/step - loss: 0.0787 - categorical_accuracy: 0.9865\n",
      "Epoch 22/30\n",
      "159571/159571 [==============================] - 808s 5ms/step - loss: 0.0775 - categorical_accuracy: 0.9865\n",
      "Epoch 23/30\n",
      "159571/159571 [==============================] - 807s 5ms/step - loss: 0.0762 - categorical_accuracy: 0.9875\n",
      "Epoch 24/30\n",
      "159571/159571 [==============================] - 799s 5ms/step - loss: 0.0759 - categorical_accuracy: 0.9867\n",
      "Epoch 25/30\n",
      "159571/159571 [==============================] - 798s 5ms/step - loss: 0.0767 - categorical_accuracy: 0.9827\n",
      "Epoch 26/30\n",
      "159571/159571 [==============================] - 796s 5ms/step - loss: 0.0740 - categorical_accuracy: 0.9832\n",
      "Epoch 27/30\n",
      "159571/159571 [==============================] - 798s 5ms/step - loss: 0.0742 - categorical_accuracy: 0.9830\n",
      "Epoch 28/30\n",
      "159571/159571 [==============================] - 800s 5ms/step - loss: 0.0732 - categorical_accuracy: 0.9846\n",
      "Epoch 29/30\n",
      "159571/159571 [==============================] - 797s 5ms/step - loss: 0.0725 - categorical_accuracy: 0.9830\n",
      "Epoch 30/30\n",
      "159571/159571 [==============================] - 804s 5ms/step - loss: 0.0712 - categorical_accuracy: 0.9832\n",
      "dict_items([('loss', [0.42496380746237966, 0.23146272175678295, 0.1587559902011183, 0.13083802576767742, 0.11766930421031578, 0.11010186297595029, 0.10492994330329007, 0.10117998845622803, 0.098272268923057737, 0.094743072917032711, 0.092413542855175959, 0.090669129519485345, 0.088866080709811265, 0.086904650954100493, 0.085573698919178653, 0.084726265771287362, 0.084124094269258654, 0.08179427756808208, 0.080820075686242651, 0.07941673340010455, 0.078679859312682232, 0.077472609031993489, 0.076223648380836451, 0.075946128851408723, 0.076710282819441464, 0.074040572391089007, 0.074246979644902317, 0.073217533257014605, 0.072498140922586815, 0.071187515057140452]), ('categorical_accuracy', [0.17182319998788215, 0.8251373987526105, 0.7244612134808599, 0.93003741444842192, 0.97044576157550644, 0.9753589343968927, 0.96797036853861484, 0.96768836687972182, 0.98093011758659343, 0.98450219545955397, 0.97700083315980946, 0.97889967672293143, 0.98381911681969902, 0.98611903215863461, 0.9871342521478319, 0.98582448683894208, 0.98259082037574919, 0.98638850303195291, 0.9887385557887779, 0.98702145291870669, 0.98654517545346321, 0.98653890798309896, 0.98752279596793824, 0.98668304473400192, 0.98267228967809328, 0.98321749974487638, 0.98304203142963442, 0.98461500140625269, 0.98299816670443996, 0.98317363496701415])])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3072\n",
    "\n",
    "def inputs():\n",
    "    padded_sequences = pad_sequences(sequences=sequences, maxlen=max_len, padding='post')\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for sequence, y_train_val in zip(padded_sequences, y_train):\n",
    "        x_list.append(sequence)\n",
    "        y_list.append(y_train_val)\n",
    "    return np.array(x_list), np.array(y_list)\n",
    "\n",
    "x_inputs, y_inputs = inputs()\n",
    "\n",
    "hist = input_model.fit(x=x_inputs, y=y_inputs, epochs=epochs, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(hist.history.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 1795s 11ms/step - loss: 0.0671 - categorical_accuracy: 0.9766\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 1644s 10ms/step - loss: 0.0665 - categorical_accuracy: 0.9752\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 1653s 10ms/step - loss: 0.0663 - categorical_accuracy: 0.9772\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 24452s 153ms/step - loss: 0.0656 - categorical_accuracy: 0.9729\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 913s 6ms/step - loss: 0.0652 - categorical_accuracy: 0.9733\n"
     ]
    }
   ],
   "source": [
    "hist = input_model.fit(x=x_inputs, y=y_inputs, epochs=5, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "y_pred = input_model.predict(x=x_inputs[0:1000, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99958831, 0.92318612, 0.99542254, 0.92499238, 0.98819602, 0.95567679]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.34259509e-03,   1.01660269e-04,   5.04357798e-04,\n",
       "          5.65093578e-05,   2.85132468e-04,   9.01358799e-05],\n",
       "       [  1.12030422e-03,   1.06385640e-04,   4.41085751e-04,\n",
       "          8.02847499e-05,   3.29066068e-04,   1.43011464e-04],\n",
       "       [  8.89734626e-02,   7.18377822e-04,   2.27558482e-02,\n",
       "          1.84875773e-03,   2.28070728e-02,   1.61362044e-03],\n",
       "       ..., \n",
       "       [  6.98640360e-04,   8.04669326e-05,   5.92404976e-04,\n",
       "          3.76475000e-05,   3.88850458e-04,   9.50718822e-05],\n",
       "       [  6.00220524e-02,   3.56231554e-04,   8.33033677e-03,\n",
       "          6.67122426e-04,   4.94798832e-03,   7.93009764e-04],\n",
       "       [  4.79182228e-02,   4.13200294e-04,   4.67344839e-03,\n",
       "          8.76317674e-04,   5.98357385e-03,   1.57622644e-03]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat_max = [np.max(y_pred[:,i]) for i in range(len(categories))]\n",
    "\n",
    "display(cat_max)\n",
    "\n",
    "display(y_pred)\n",
    "\n",
    "y_pred_labels = np.zeros((y_pred.shape[0], y_pred.shape[1]))\n",
    "\n",
    "for cmax, i in zip(cat_max, range(len(categories))):\n",
    "    y_pred_labels[y_pred[:,i] >= cmax, i] = 1\n",
    "    y_pred_labels[y_pred[:,i] < cmax, i] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_pred_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89200000000000002"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_train[0:1000,:], y_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153164,)\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('input/test.csv')\n",
    "\n",
    "x_test = df_test['comment_text']\n",
    "\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164\n"
     ]
    }
   ],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(x_test)\n",
    "padded_sequences_test = pad_sequences(sequences=sequences_test, maxlen=max_len, padding='post')\n",
    "\n",
    "print(len(padded_sequences_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = input_model.predict(padded_sequences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153164, 6)\n",
      "[0.99993396, 0.99841452, 0.99944335, 0.99987817, 0.99900961, 0.99006492]\n",
      "[7.2540293e-05, 5.5869591e-06, 7.8540055e-05, 2.1909384e-06, 1.3558042e-05, 3.8137257e-06]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_pred.shape)\n",
    "\n",
    "test_cat_max = [np.max(y_test_pred[:,i]) for i in range(len(categories))]\n",
    "\n",
    "print(test_cat_max)\n",
    "\n",
    "test_cat_min = [np.min(y_test_pred[:,i]) for i in range(len(categories))]\n",
    "\n",
    "print(test_cat_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153164, 6)\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_labels = np.zeros((y_test_pred.shape[0], y_test_pred.shape[1]))\n",
    "\n",
    "print(y_test_pred_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  1.,  0.,  0.,  0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshold = [0.90, 0.90, 0.90, 0.90, 0.90, 0.90]\n",
    "\n",
    "for t, i in zip(threshold, range(len(threshold))):\n",
    "    y_test_pred_labels[y_test_pred[:,i] >= t, i] = 1\n",
    "\n",
    "display(y_test_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153164, 7)\n"
     ]
    }
   ],
   "source": [
    "df_submission = pd.DataFrame()\n",
    "df_submission['id'] = df_test['id']\n",
    "for c, i in zip(categories, range(len(categories))):\n",
    "    df_submission[c] = y_test_pred_labels[:, i]\n",
    "\n",
    "    \n",
    "print(df_submission.shape)\n",
    "\n",
    "df_submission.to_csv('output/lstm_glve_4B_07_300_35e_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
